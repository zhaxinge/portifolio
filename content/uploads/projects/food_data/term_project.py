# -*- coding: utf-8 -*-
"""Term Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18xaeuF86_e9v7we8dliWsVsfd3LWmirA

#Introduction
Hi there! Welcome to Xinge, Tianyu, Xiaozhe's CIS 545 final project from Spring 2022. 

Throughout this notebook we'll be trying to predict OpenFoodFacts's public dataset. We intend to find the correlation among each feature of the food by vis ualization and the logic behind the nutrient score. We also want to divide foods into different categories by using the clusters method to find out the common characteristic of different foods.We'll walk you through the entire process by downloading and acquiring preliminary datasets, extracting features, conducting EDA, and doing modelling.

Let's begin!

# Libraries and Setup

Run the following cells to set up the notebook.

The data set is quoted from [Kaggle](https://www.kaggle.com/openfoodfacts/world-food-facts).
We will use the food data set which consists of ingredients and nutritions
information of those food. This data set was made by 5000+ contributors having added 600 000+ products from 150 countries using Phone apps or cameras to scan barcodes and upload pictures of products and their labels. In this data set, there are 163 features.

# Mount
We utilized the below drive for much our analysis. You can access the files and modify your respective google drive paths if you wish to run this notebook


*  Click on this google drive [link for drive](https://drive.google.com/drive/u/1/folders/1KEfjr3pJmQmMlXo9-Rmt4bITk5SK-3Wa).
*  Right click the "545finalproject", choose "MyDrive" and click "Add ShortCut".
*  After finishing your session, you may delete the "545finalproject" folder from the private google drive
"""

from google.colab import drive
drive.mount('/content/drive')

"""# Import"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
PATH = "/content/drive/MyDrive/Colab Notebooks/CIS545project"

main_df = pd.read_csv(PATH+"/en.openfoodfacts.org.products.tsv", sep="\t")

"""# Preprocess

The data set contains a single table, FoodFacts, in CSV form in FoodFacts.csv. It has 356027 food instances and features including product names, countries, ingredients, allergens, nutrition components, etc.
"""

main_df.head(5)

main_df.columns.values

"""### Drop unnesssary columns

Since we intend to find the correlation among each feature of the food by vis ualization and the logic behind the nutrient score, we only need to keep the columns of nutrient content.





"""

drop_index = list(main_df.columns.values).index("energy_100g")
clean_df = main_df[main_df.columns.values[drop_index:]]
clean_df.columns.values

"""### Drop columns with too many of NaN values

We first calculated the mean percantage of NaN values in each column.
"""

null_col_percent = clean_df.isnull().mean(axis=0)
null_col_percent

"""We decided to drop columns with more than 90% of NaN values."""

drop_percent = 0.9
clean_df = clean_df.drop(columns=null_col_percent[null_col_percent>drop_percent].index)
clean_df.head(5)

clean_df.columns.values

"""Finally, drop any rows with NaN values."""

clean_df = clean_df.dropna()
clean_df

"""# Correlation of Food ingredient Variables

Create a **correlation table**, **boxplot** and **heatmap** for the features in clean_df. Intuitively, there should be a roughly linear relationship between nutrition scores and nutrition ingredients. We also speculate that nutrition scores are related to the content of nutrition ingredients.

We can see that 'fat' is positively correlated with 'saturated fat', 'protein' and nutrition scores. 'carbonhydrates' is positively correlated with 'sugars', 'fiber' and nutrition score. Interestingly, 'protein' is negatively correlated with 'sugars'. **Nutrition scores** are **positively** correlated with **'fat', 'satureted fat', 'carbonhydrates', 'sugars', 'proteins'** .

It is worth noticing that **some features are correlated, like energy and carbonhydrates** and this may affect our linear regression training. 
"""

(clean_df["nutrition-score-fr_100g"] == clean_df["nutrition-score-uk_100g"]).mean()

"""We found the value of **"nutrition-score-fr_100g"** and **"nutrition-score-uk_100g"** are basically the same. So we decided to use **"nutrition-score-uk_100g"** ."""

X = clean_df[clean_df.columns.values[:-2]]
y = clean_df["nutrition-score-uk_100g"]

X=(X-X.min())/(X.max()-X.min())
# X=(X-X.mean())/X.std()
plt.figure(figsize=(35,10))
X.boxplot()

corrMatrix = X.corr()
plt.figure(figsize=(15,10))
plt.title("Correlation Matrix")
sns.heatmap(corrMatrix, vmax=1, vmin=-1, center=0, cmap='vlag')
plt.show()

"""# Split into Features and Label

We split data into train and test sets
"""

X

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""---

# Regression


Many types of regression models are available, and it is crucial to choose the most suitable technique based on independent and dependent variables, data dimensions, and other necessary data features. In this section, we are trying to get a formula of nutrition score from our data set. Intuitively, there should be a linear relationship between nutrition score and nutrition ingredients. In other words, we should be able to predict nutrition score with a polynomial expression.

If we use a regular linear regression without regularization, the weights would be skewed to overfit the data set. This is true, by the previous section (covarience matrix), highly correlated features exist in our data set.To avoid that, we use elastic net regression model to predict nutrition score.

**Ridge regularization** works well when there are many large parameters of around the same value – and may work slightly better if there is multicollinearity. 

**Lasso regularization** tends to work well if there are a few significant parameters and the others aren’t significant. This should be true in our model. Some ingredients are more important than others. 

**Polynomial regression** is to model a non-linear relationship between the independent and dependent variables (technically, between the independent variable and the conditional mean of the dependent variable). This is similar to the goal of nonparametric regression, which aims to capture non-linear regression relationships.An advantage is that the inferential framework of multiple regression can be used. So we use it to test the effect of adding dimensions on the regression results.

## Ridge Regression
"""

from sklearn.linear_model import Ridge

alphas = [0.1*n for n in range(1,11)]
scores = []
for alpha in alphas:
  ridge_model = Ridge(alpha)
  ridge_model.fit(X_train,y_train)
  y_pred = ridge_model.predict(X_test)
  scores.append(ridge_model.score(X_test,y_test))
  print("--------------- alpha =",alpha,"-------------------")
  print("score:",ridge_model.score(X_test,y_test))

plt.figure(figsize=((10,5)))
plt.title("Score vs Alpha")
plt.xlabel("alpha")
plt.ylabel("score")
plt.plot(alphas,scores)
plt.show()

print("when alpha = ", 0.1*(np.argmax(scores)+1), ", rigde regression get best score =", max(scores))

"""## Lasso Regression"""

from sklearn.linear_model import Lasso

alphas = [0.1*n for n in range(1,11)]
scores = []
for alpha in alphas:
  lasso_model = Lasso(alpha)
  lasso_model.fit(X_train,y_train)
  y_pred = lasso_model.predict(X_test)
  scores.append(lasso_model.score(X_test,y_test))
  print("--------------- alpha =",alpha,"-------------------")
  print("score:",lasso_model.score(X_test,y_test))

plt.figure(figsize=((10,5)))
plt.title("Score vs Alpha")
plt.xlabel("alpha")
plt.ylabel("score")
plt.plot(alphas,scores)
plt.show()

print("when alpha = ", 0.1*(np.argmax(scores)+1), ", lasso regression get best score =", max(scores))

"""## Elastic Net"""

from sklearn.linear_model import ElasticNet

alphas = [0.1*n for n in range(0,11)]
l1_ratios = [0.1*n for n in range(0,11)]
scores = []
max_r = 0
max_alpha = 0
max_score = 0
for alpha in alphas:
  for r in l1_ratios:
    en_model = ElasticNet(alpha = alpha,l1_ratio=r)
    en_model.fit(X_train,y_train)
    y_pred = en_model.predict(X_test)
    score = en_model.score(X_test,y_test)
    scores.append(score)
    print("--------------- alpha =",alpha,",l1_ratio =",r,"-------------------")
    print("score:",score)

    if score > max_score:
      max_score = score
      max_r = r
      max_alpha = alpha

print("when alpha = ", max_alpha, "l1_ratio =",r, ",  elastic net get best score =", max(scores))

"""## Polynomial Regression"""

from sklearn.preprocessing import PolynomialFeatures

poly = PolynomialFeatures(degree=2, include_bias=False)
poly_X_train = poly.fit_transform(X_train)
poly_X_test = poly.transform(X_test)

from sklearn.linear_model import LinearRegression

lr_model = LinearRegression()
lr_model.fit(poly_X_train, y_train)
score = lr_model.score(poly_X_test,y_test)

print("poly regression score =", score)

"""## Conclusion of Linear Regression Section

The result alpha of Ridge regression tends to be high. On the contrary, the result alpha of Lasso regression tends to be low.This may be due to there are many large parameters of around the same value in this dataset.The result of Elastic Net also confirms that. Elastic net gets the best score when alpha = 0.

Again we can see that the regression model derived by up-dimensioning the data does not work well, indicating that although our data set is relatively simple, the data distribution is relatively dispersed and does not need to be up-dimensioned.

#PCA and Complex Nonlinear Methods

## PCA and Data Visualization

We would like to apply PCA first to the dataset before we train the complex method since it will take much more time than training the linear regression method
"""

from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
pca = PCA(n_components=8)
X_pca = StandardScaler().fit_transform(X)
X_pca = pca.fit_transform(X_pca)

X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2,random_state=42)

pc_vs_variance = np.cumsum(pca.explained_variance_ratio_)

plt.plot(pc_vs_variance)
pc_vs_variance

import plotly.express as px
from sklearn.decomposition import PCA


features = ['energy_100g', 'fat_100g', 'saturated-fat_100g', 'trans-fat_100g',
       'cholesterol_100g', 'carbohydrates_100g', 'sugars_100g', 'fiber_100g',
       'proteins_100g', 'salt_100g', 'sodium_100g', 'vitamin-a_100g',
       'vitamin-c_100g', 'calcium_100g', 'iron_100g']

pca = PCA()
components = pca.fit_transform(X[features])
labels = {
    str(i): f"PC {i+1} ({var:.1f}%)"
    for i, var in enumerate(pca.explained_variance_ratio_ * 100)
}

fig = px.scatter_matrix(
    components,
    labels=labels,
    dimensions=range(10),

)
fig.update_traces(diagonal_visible=False)
fig.show()

"""Through visual analysis, we can see that the principal components abstracted by PC1, PC2, PC3 can more successfully represent the overall data. Therefore, we want to choose these three components to go further visualization to see their structure and distribution."""

import plotly.express as px
from sklearn.decomposition import PCA



pca = PCA(n_components=3)
components = pca.fit_transform(X)

total_var = pca.explained_variance_ratio_.sum() * 100

fig = px.scatter_3d(
    components, x=0, y=1, z=2, 
    title=f'Total Explained Variance: {total_var:.2f}%',
    labels={'0': 'PC 1', '1': 'PC 2', '2': 'PC 3'}
)
fig.show()

!pip install jupyter-dash

import plotly.express as px
from jupyter_dash import JupyterDash
import dash_core_components as dcc
import dash_html_components as html
from dash.dependencies import Input, Output
app = JupyterDash(__name__)

app.layout = html.Div([
    html.H4("Visualization of PCA's explained variance"),
    dcc.Graph(id="graph"),
    html.P("Number of components:"),
    dcc.Slider(
        id='slider',
        min=2, max=10, value=3, step=1)
])


@app.callback(
    Output("graph", "figure"), 
    Input("slider", "value"))
def update_figure(n_components):

    df = X[features]
    pca = PCA(n_components=n_components)
    components = pca.fit_transform(df)

    var = pca.explained_variance_ratio_.sum() * 100

    labels = {str(i): f"PC {i+1}" 
              for i in range(n_components)}
    labels['color'] = 'Median Price'

    fig = px.scatter_matrix(
        components,
        dimensions=range(n_components),
        labels=labels,
        title=f'Total Explained Variance: {var:.2f}%')
    fig.update_traces(diagonal_visible=False)
    return fig

app.run_server(mode='inline')

"""## Conclusion of Principle Component Analysis
We do principle analysis with n_components = 3. With px.scatter_3d, we visualize the data and notice that our data was likely ball-shaped.

##Neural Network
"""

from sklearn.neural_network import MLPClassifier
clf = MLPClassifier(solver='adam', alpha=0.1,
                  hidden_layer_sizes=(256,50),max_iter=50, random_state=42)
clf.fit(X_train, y_train)

score = clf.score(X_test,y_test)

print("neural network score =", score)

"""##Gradient Boosting"""

from sklearn.ensemble import GradientBoostingClassifier

clf = GradientBoostingClassifier(n_estimators=20, learning_rate=0.1,
   max_depth=1, random_state=42,tol=0.5).fit(X_train, y_train)

score = clf.score(X_test,y_test)

print("Gradient boosting score =", score)

"""## Conclusion of Complex Nonlinear Methods

It can be clearly seen that the nonlinear method does not give as good results as the linear regression. But since what we want to achieve in the regression phase is to be able to successfully help people predict nutrition scores based on different nutritional elements. We believe that the linear regression model has helped us to achieve this largely, and that conducting a neural network is time consuming so we did not explore models such as neural net any further.

---

# Clustering

We did regression using the nutrition table to predict the nutrition score. Now we want to investigate how can we divide the food into different category based on their nutrition table to make life easier for people who decide to live on a healthy life by having different types of nutrient and do not want  to read the nutrition table. We can just recommend different types of food every day to those people. We will try the K means model and guassian mixture model.

##K Means Model

K means algorithm takes number of cluster input and then classify the data into those number of clusters. It firstly initialize one center point for each of cluster. We choose to initialize randomly here. Then it runs for some iterations until the center point is not changing dramatically and the distortion is not changing or reaches the max number of iteration.
"""

from sklearn.cluster import KMeans

errors = []
# we think max number of clusters to be 25 is reasonable for food categories,like sugar, meat, nuts and so on
max_clusters = 25
for i in range(1,max_clusters+1):
  km = KMeans(n_clusters=i,init='random',max_iter=400,random_state=42)
  km.fit(X)
  errors.append(km.inertia_)

plt.plot( errors, marker='x')
plt.xlabel('number of clusters')
plt.ylabel('Distortion')
plt.show()

"""When number of clusters equals to 10, the distortion will not decrease too much as the number of clusters increase. In order to prevent overfitting, we choose the number of clusters to be 10. Then we get our prediction for the cluster of each food, store it in column category."""

from copy import deepcopy
km = KMeans(n_clusters=10,
              init='random',
           
              max_iter=400,
              random_state=42)
km.fit(X)
Kmeans_X = deepcopy(X)
Kmeans_X["category"] = km.predict(X)

ds=Kmeans_X.sample(15000)

"""Now we want to investigate how are those clusters correlated with each other. We start with drawing the carbohydrates_100g and proteins_100g of each food and color them in category type. carbohydrates and proteins are fundamental nutricient of food and people have to absorb certain amount of them each day."""

da=ds
plt.figure(figsize=(8,8))
plt.style.use('default')
#plt.set_facecolor(pal_bgrd[0])

sns.scatterplot(x=da['carbohydrates_100g'], 
                y=da['proteins_100g'], #'fiber_100g'
                hue=da['category'],
                hue_order=[0,1,2,3,4,5,6,7,8,9],
                legend='full')
                #palette=legend) 
plt.title("the distribution of products 'A' according to the categories")

# credit to https://plotly.com/python/pca-visualization/
import plotly.express as px
from sklearn.decomposition import PCA



pca = PCA(n_components=3)
components = pca.fit_transform(da)

total_var = pca.explained_variance_ratio_.sum() * 100

fig = px.scatter_3d(
    components, x=0, y=1, z=2, color=da['category'],
    title=f'Total Explained Variance: {total_var:.2f}%',
    labels={'0': 'PC 1', '1': 'PC 2', '2': 'PC 3'}
)
fig.show()

"""Through visual analysis, we can see that the principal components of the different categories derived from k-means are mostly different, which proves the effectiveness of the classifier."""

# credit to https://plotly.com/python/pca-visualization/
import plotly.express as px
from sklearn.decomposition import PCA
from sklearn import datasets
from sklearn.preprocessing import StandardScaler

df = da[['carbohydrates_100g','proteins_100g','fat_100g','category']]

features = ['carbohydrates_100g','proteins_100g','fat_100g']
pca = PCA(n_components=2)
components = pca.fit_transform(df)

loadings = pca.components_.T * np.sqrt(pca.explained_variance_)

fig = px.scatter(components, x=0, y=1, color=df['category'])



for i, feature in enumerate(features):
    fig.add_shape(
        type='line',
        x0=0, y0=0,
        x1=loadings[i, 0],
        y1=loadings[i, 1]
    )
    fig.add_annotation(
        x=loadings[i, 0],
        y=loadings[i, 1],
        ax=0, ay=0,
        xanchor="center",
        yanchor="bottom",
        text=feature,
    )
fig.show()

"""By showing the data on the two principal components, we can see that for **'carbohydrates', 'proteins', 'fat',** the correlation between fat and protein is high, while having high carbohydrates is not necessarily rich in proteins and fat, which is reasonable, for example, honey in catergory 4 has that  characteristic.

Then we plot fat_100g, saturated-fat_100g,fiber_100g,carbohydrates_100g, and proteins_100g with one to one relationship in the same way above. Those nutricient elements basically contain all elements people need to absorb each day.
"""

sns.set(font_scale=1)
plt.figure(figsize=(4, 4))

sns.pairplot(ds.loc[:,['fat_100g', 'saturated-fat_100g', 'fiber_100g', 'carbohydrates_100g', 'proteins_100g','category' ]], 
             hue='category',
             hue_order=[0,1,2,3,4,5,6,7,8,9],
             height=2.5
     
           )

plt.title('Mains nutriments according to nutriment_grade_fr', loc='right')

"""
In the section below, we created the word clouds for each cluster to visualize what is inside each cluster. This should be able to reveal the hidden product type. """

from wordcloud import WordCloud

Kmeans_X["product_name"] = main_df.loc[Kmeans_X.index, "product_name"]
plt.figure(figsize=(10,20))
for cat in range(10):
  words = Kmeans_X[Kmeans_X.category == cat][["product_name"]].apply(lambda x: str(x).lower().split())
  words = words.apply(pd.Series).stack().reset_index(drop=True)
  wordcloud = WordCloud(max_font_size=40, max_words=30, background_color="white")
  freqs = words.value_counts()
  wordcloud.generate_from_frequencies(freqs)
  
  ax = plt.subplot(5, 2, cat+1)
  ax.set_title("Catgory "+str(cat))
  plt.grid(False)

  plt.imshow(wordcloud, interpolation='bilinear')

"""##Guassian Mixture Model

Guassian Mixture Model is like a generalization of K Means model. K means only gives the hard probability of food. So each food can only belong to one category with 100% probability. Guassian Mixture Model uses certain amount of Normal Distribution to represent each category, and it uses all those normal distribution combined with weight assign to each other to represent each food. We think some food might have some different characteristics from different food types. For example, hamburgers contain both meat and bread, so it might belong to meat type and bread type with some weight. So GMM might be a good model to use.

We have 10 clusters from K Means, so we want to choose 10 normal distributions to represent those 10 clusters in GMM model.
"""

from sklearn.mixture import GaussianMixture

GMM = GaussianMixture(n_components=10,
                        covariance_type="full",
                        random_state=42,
         
                        max_iter=400,
                        init_params="kmeans")
GMM.fit(X)

GMM_X = deepcopy(X)
GMM_X["category"] = GMM.predict(X)

ds=GMM_X.sample(15000)

da=ds
plt.figure(figsize=(8,8))
plt.style.use('default')
#plt.set_facecolor(pal_bgrd[0])

sns.scatterplot(x=da['carbohydrates_100g'], 
                y=da['proteins_100g'], #'fiber_100g'
                hue=da['category'],
                hue_order=[0,1,2,3,4,5,6,7,8,9],
                legend='full')
                #palette=legend) 
plt.title("the distribution of products 'A' according to the categories")

"""##Principle Component Analysis
We do principle analysis with n_components = 3. With px.scatter_3d, we visualize the data and notice that our data was likely ball-shaped.
"""

# credit to https://plotly.com/python/pca-visualization/
import plotly.express as px
from sklearn.decomposition import PCA



pca = PCA(n_components=3)
components = pca.fit_transform(da)

total_var = pca.explained_variance_ratio_.sum() * 100

fig = px.scatter_3d(
    components, x=0, y=1, z=2, color=da['category'],
    title=f'Total Explained Variance: {total_var:.2f}%',
    labels={'0': 'PC 1', '1': 'PC 2', '2': 'PC 3'}
)
fig.show()

# credit to https://plotly.com/python/pca-visualization/
import plotly.express as px
from sklearn.decomposition import PCA
from sklearn import datasets
from sklearn.preprocessing import StandardScaler

df = da[['carbohydrates_100g','proteins_100g','fat_100g','category']]

features = ['carbohydrates_100g','proteins_100g','fat_100g']
pca = PCA(n_components=2)
components = pca.fit_transform(df)

loadings = pca.components_.T * np.sqrt(pca.explained_variance_)

fig = px.scatter(components, x=0, y=1, color=df['category'])

for i, feature in enumerate(features):
    fig.add_shape(
        type='line',
        x0=0, y0=0,
        x1=loadings[i, 0],
        y1=loadings[i, 1]
    )
    fig.add_annotation(
        x=loadings[i, 0],
        y=loadings[i, 1],
        ax=0, ay=0,
        xanchor="center",
        yanchor="bottom",
        text=feature,
    )
fig.show()

"""Compared with k-means, we can see that for **'carbohydrates', 'proteins', 'fat',** the correlation between fat and protein is higher, while having high carbohydrates is relatively common. There are also cases that contain only fat and protein."""

sns.set(font_scale=1)
plt.figure(figsize=(4, 4))

sns.pairplot(ds.loc[:,['fat_100g', 'saturated-fat_100g', 'fiber_100g', 'carbohydrates_100g', 'proteins_100g','category' ]], 
             hue='category',
             hue_order=[0,1,2,3,4,5,6,7,8,9],
             height=2.5
     
           )

plt.title('Mains nutriments according to nutriment_grade_fr', loc='right')

"""

In the section below, we created the word clouds for each cluster to visualize what is inside each cluster. Compared with the k-means cluster, we can see that the items within each cluster products have changed considerably."""

GMM_X["product_name"] = main_df.loc[GMM_X.index, "product_name"]
plt.figure(figsize=(10,20))
for cat in range(10):
  words = GMM_X[GMM_X.category == cat][["product_name"]].apply(lambda x: str(x).lower().split())
  words = words.apply(pd.Series).stack().reset_index(drop=True)
  wordcloud = WordCloud(max_font_size=40, max_words=30, background_color="white")
  freqs = words.value_counts()
  wordcloud.generate_from_frequencies(freqs)
  
  ax = plt.subplot(5, 2, cat+1)
  ax.set_title("Catgory "+str(cat))
  plt.grid(False)

  plt.imshow(wordcloud, interpolation='bilinear')

"""## Conclusion of Clustering
It can be clearly seen that the clusters derived from different cluster methods are very different. This may be related to the fact that the initial points of the k-mean itself are random. However, we successfully confirmed the validity of the cluster through PCA visualization, especially the cluster results in the visualization of the 3 principal components, we can clearly see that there is basically no similarity between different cluster data.

# Conclusion 

In our project, we first processed and explored the dataset FoodFacts on kaggle, and identified the nutrients associated with the nutrition score as the basis for further training in the future. Then we find the relationship between nutrition score and nutrition components by regression and cluster respectively.

By comparing linear regression models, we can see each model reveals some features in dataset with practical meanings. The best score of lasso regression shows that there are many large parameters of around the same value in this dataset. That reveals the intuition that the contribution of different nutrients to the nutrition score is about the same. In exploring the nonlinear methods, all models do not predict well. To improve our model, we need to do more hyperparameter tuning and may also try some method like cross validation.

In the section of clustering, the clusters of the K-Means model seem to be more equal in size and some of the clusters do not look natural. The clusters of Gaussian Mixture Model seem to be more scattered. However, K-Means model still has many advantages like computationally cheap and if we knew that our data was likely ball-shaped, then it would be better to use K-Means model.

By using PCA visualization, we proved the validity of our model. Also, we explore the most representative features like  'carbohydrates', 'proteins', 'fat' and use PCA to visuallize the instances in our cloud data. We find that  different clouds have different results. Basically, the correlation between fat and protein is high, while having high carbohydrates is not necessarily rich in proteins and fat. Compared with k-means, using a more complex model like Gaussian Mixture Model, the correlation between fat and protein is higher, and features are more sparse in each cluster, which means the model better fits the training dataset.

It is worth noticing that some words are very common and exist in many clusters. Also, to improve our wordcloud, we might need to deal with features inside the dataset.This natural clustering may be useful for future supervised machine learning and provide us some hidden information of classifying food basing on their nutrition components.

Though these models and visualization showed some interesting practical significance, the perception of models still need to be improved. To improve our model, we need to improve the hyperparameter and try cross validation in nonlinear regression models. We should also be aware that our data may not provide enough features thus making the types inseparable. Using a more complex model may help us separate different types of products but also increases the risk of overfitting.
"""